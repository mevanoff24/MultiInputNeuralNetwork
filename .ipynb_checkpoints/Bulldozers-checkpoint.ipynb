{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "warnings.filterwarnings('ignore', category=DataConversionWarning)\n",
    "\n",
    "def transform_date(df, field_name, drop=True):\n",
    "    field = df[field_name]\n",
    "    if not np.issubdtype(field, np.datetime64):\n",
    "        df[field_name] = field = pd.to_datetime(field, infer_datetime_format=True)\n",
    "    target_pre = re.sub('[Dd]ate$', '', field_name)\n",
    "    for i in ('Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start'):\n",
    "        df[target_pre + i] = getattr(field.dt, i.lower())\n",
    "    df[target_pre + 'Elapsed'] = field.astype(np.int64) // 10**9\n",
    "    if drop:\n",
    "        df.drop(field_name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def create_category_fields(df, is_train=True, train_df=None):  \n",
    "    if is_train:\n",
    "        for col_name, data in df.items():\n",
    "            if is_string_dtype(data):\n",
    "                df[col_name] = data.astype('category').cat.as_ordered()\n",
    "    else:\n",
    "        for col_name, data in df.items():\n",
    "            if (col_name in train_df.columns) and (train_df[col_name].dtype.name == 'category'):\n",
    "                df[col_name] = pd.Categorical(data, categories=train_df[col_name].cat.categories, ordered=True)\n",
    "\n",
    "\n",
    "\n",
    "def fix_missing(df, col, name, na_dict):\n",
    "    if is_numeric_dtype(col):\n",
    "        if pd.isnull(col).sum() or (name in na_dict):\n",
    "            df[name + '_na'] = pd.isnull(col)\n",
    "            filler = na_dict[name] if name in na_dict else col.median()\n",
    "            df[name] = col.fillna(filler)\n",
    "            na_dict[name] = filler\n",
    "    return na_dict\n",
    "\n",
    "\n",
    "def numericalize(df, col, name, max_n_cat):\n",
    "    if not is_numeric_dtype(col) and ( max_n_cat is None or col.nunique() > max_n_cat):\n",
    "        df[name] = col.cat.codes + 1\n",
    "\n",
    "\n",
    "def scale_vars(df, mapper):\n",
    "    if mapper is None:\n",
    "        map_f = [([n],StandardScaler()) for n in df.columns if is_numeric_dtype(df[n])]\n",
    "        mapper = DataFrameMapper(map_f).fit(df)\n",
    "    df[mapper.transformed_names_] = mapper.transform(df)\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def preprocessing(df, y_fld, skip_flds=None, do_scale=False, na_dict=None,\n",
    "            preproc_fn=None, max_n_cat=None, mapper=None):\n",
    "\n",
    "    if not skip_flds: \n",
    "        skip_flds = []\n",
    "    df = df.copy()\n",
    "    if preproc_fn: \n",
    "        preproc_fn(df)\n",
    "\n",
    "    y = df[y_fld].values\n",
    "    df.drop(skip_flds + [y_fld], axis=1, inplace=True)\n",
    "\n",
    "    if na_dict is None: \n",
    "        na_dict = {}\n",
    "    for n,c in df.items(): \n",
    "        na_dict = fix_missing(df, c, n, na_dict)\n",
    "    if do_scale: \n",
    "        mapper = scale_vars(df, mapper)\n",
    "    for n,c in df.items(): \n",
    "        numericalize(df, c, n, max_n_cat)\n",
    "\n",
    "    res = [pd.get_dummies(df, dummy_na=True), y, na_dict]\n",
    "\n",
    "    if do_scale: \n",
    "        res = res + [mapper]\n",
    "    return res\n",
    "\n",
    "def set_rf_samples(n):\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n))\n",
    "\n",
    "def reset_rf_samples():\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YearMade saleElapsed SalesID MachineID saleDay saleDayofyear age'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_numeric_dtype\n",
    "\n",
    "\n",
    "dep = 'SalePrice'\n",
    "PATH = \"data/bulldozers/\"\n",
    "df_raw = pd.read_feather('tmp/bulldozers-raw')\n",
    "keep_cols = list(np.load('tmp/keep_cols.npy'))\n",
    "\n",
    "df_raw.loc[df_raw.YearMade<1950, 'YearMade'] = 1950\n",
    "df_raw['age'] = df_raw.saleYear-df_raw.YearMade\n",
    "df_raw = df_raw[keep_cols+['age', dep]].copy()\n",
    "df_indep = df_raw.drop(dep,axis=1)\n",
    "\n",
    "n_valid = 12000\n",
    "n_trn = len(df_raw)-n_valid\n",
    "\n",
    "\n",
    "cat_flds = [n for n in df_indep.columns if df_raw[n].nunique()<n_trn/50]\n",
    "' '.join(cat_flds)\n",
    "\n",
    "for o in ['saleElapsed', 'saleDayofyear', 'saleDay', 'age', 'YearMade']: cat_flds.remove(o)\n",
    "[n for n in df_indep.drop(cat_flds,axis=1).columns if not is_numeric_dtype(df_raw[n])]\n",
    "\n",
    "\n",
    "for n in cat_flds: df_raw[n] = df_raw[n].astype('category').cat.as_ordered()\n",
    "\n",
    "cont_flds = [n for n in df_indep.columns if n not in cat_flds]\n",
    "' '.join(cont_flds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw[cat_flds+cont_flds+[dep]]\n",
    "df, y, nas, mapper = preprocessing(df_raw, 'SalePrice', do_scale=True)\n",
    "\n",
    "val_idx = list(range(n_trn, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(x,y): return np.sqrt(((x-y)**2).mean())\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "emb_c = {n: len(c.cat.categories)+1 for n,c in df_raw[cat_flds].items()}\n",
    "\n",
    "emb_szs = [(c, min(50, (c+1)//2)) for _,c in emb_c.items()]\n",
    "metrics=[rmse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.init import kaiming_normal\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.optim import Adam, RMSprop\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "def split_by_idx(idxs, *a):\n",
    "    mask = np.zeros(len(a[0]),dtype=bool)\n",
    "    mask[np.array(idxs)] = True\n",
    "    return [(o[mask],o[~mask]) for o in a]\n",
    "\n",
    "\n",
    "def init_embeddings(x):\n",
    "    x = x.weight.data\n",
    "    value = 2 / (x.size(1) + 1)\n",
    "    x.uniform_(-value, value)\n",
    "    \n",
    "class StructuredData(object):\n",
    "    def __init__(self, df, y, cat_flds, cont_flds, val_index=None, batch_size=32, shuffle=False, num_workers=1):\n",
    "        self.val_index = val_index\n",
    "        if val_index:\n",
    "            ((val_df, df), (y_val, y)) = split_by_idx(val_index, df, y)\n",
    "\n",
    "        train_dataset = StructuredDataSet(df[cat_flds], df[cont_flds], y)\n",
    "        \n",
    "        if shuffle:\n",
    "            train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "        else:\n",
    "            train_sampler = None\n",
    "            \n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=(train_sampler is None), \n",
    "                                       sampler=train_sampler, num_workers=num_workers)\n",
    "        \n",
    "        if val_index:\n",
    "            validation_dataset = StructuredDataSet(val_df[cat_flds], val_df[cont_flds], y_val)\n",
    "            self.validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, \n",
    "                                            num_workers=num_workers)\n",
    "        else:\n",
    "            self.validation_loader = None\n",
    "            \n",
    "    def get_data(self):  \n",
    "        if self.val_index:\n",
    "            return self.train_loader, self.validation_loader\n",
    "        else:\n",
    "            return self.train_loader  \n",
    "    \n",
    "class StructuredDataSet(Dataset):\n",
    "    def __init__(self, cats, conts, y):\n",
    "        self.cats = np.asarray(cats, dtype=np.int64)\n",
    "        self.conts = np.asarray(conts, dtype=np.float32)\n",
    "        self.N = len(y)\n",
    "        y = np.zeros((n,1)) if y is None else y[:,None]\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "            \n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.cats[idx], self.conts[idx], self.y[idx]]\n",
    "    \n",
    "\n",
    "     \n",
    "class MultiInputNN(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, sizes, drops, y_range=None, use_bn=False, f=F.relu):\n",
    "        super().__init__()\n",
    "        # embedding layers\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(insize, outsize) for insize, outsize in emb_szs])\n",
    "        for layer in self.embeddings:\n",
    "            init_embeddings(layer)\n",
    "        self.num_categorical = sum([layer.embedding_dim for layer in self.embeddings])\n",
    "\n",
    "        self.num_numerical = n_cont\n",
    "        # linear layers\n",
    "        sizes = [self.num_categorical + self.num_numerical] + sizes\n",
    "        self.linear = nn.ModuleList([nn.Linear(sizes[i], sizes[i+1]) for i in range(len(sizes)-1)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(size) for size in sizes[1:]])\n",
    "        for layer in self.linear:\n",
    "            kaiming_normal(layer.weight.data)\n",
    "        # dropout layers\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drop_out = [nn.Dropout(drop) for drop in drops]\n",
    "        # output layer\n",
    "        self.output = nn.Linear(sizes[-1], 1)\n",
    "        kaiming_normal(self.output.weight.data)\n",
    "        self.f = f\n",
    "        self.bn = nn.BatchNorm1d(self.num_numerical)\n",
    "        self.use_bn = use_bn\n",
    "        self.y_range = y_range\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if self.num_categorical > 0:\n",
    "            X = [emb_layer(x_cat[:,i]) for i, emb_layer in enumerate(self.embeddings)]\n",
    "            X = torch.cat(X, dim=1)\n",
    "            X = self.emb_drop(X)\n",
    "        if self.num_numerical > 0:\n",
    "            X2 = self.bn(x_cont)\n",
    "            X = torch.cat([X, X2], dim=1) if self.num_categorical != 0 else X2\n",
    "        for linear, drop, norm in zip(self.linear, self.drop_out, self.bns):\n",
    "            X = self.f(linear(X))\n",
    "            if self.use_bn: \n",
    "                X = norm(X)\n",
    "            X = drop(X)\n",
    "        X = self.output(X)\n",
    "        if self.y_range:\n",
    "            X = F.sigmoid(X)\n",
    "            X = X * (self.y_range[1] - self.y_range[0])\n",
    "            X = X + self.y_range[0]\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def fit(self, train_loader, learning_rate=1e-3, batch_size=64, epochs=1, val_loader=None, metrics=None, save=False, save_path='tmp/checkpoint.pth.tar', \n",
    "                    pre_saved=False):\n",
    "\n",
    "        loss = nn.MSELoss()\n",
    "        optimizer = RMSprop(self.parameters(), lr=learning_rate)\n",
    "        n_batches = int(train_loader.dataset.N / train_loader.batch_size)\n",
    "\n",
    "        if pre_saved:\n",
    "            checkpoint = torch.load(save_path)\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            self.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print('...restoring model...')\n",
    "            \n",
    "        begin = True\n",
    "        for epoch_ in range(epochs):        \n",
    "            if pre_saved:      \n",
    "                if begin:\n",
    "                    epoch = start_epoch\n",
    "                    begin = False\n",
    "            else:\n",
    "                epoch = epoch_\n",
    "            epoch += 1\n",
    "\n",
    "            epoch_loss = 0.\n",
    "            # train phase \n",
    "            self.train()\n",
    "            for i, (batch_cat, batch_cont, batch_y) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                batch_cat, batch_cont, batch_y = Variable(batch_cat), Variable(batch_cont), Variable(batch_y)\n",
    "\n",
    "                y_hat = self.forward(batch_cat, batch_cont)\n",
    "                l = loss(y_hat, batch_y)\n",
    "                epoch_loss += l.data[0]\n",
    "                \n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if i != 0 and i % 2000 == 0:\n",
    "                    print('iteration: {} of n_batches: {}'.format(i, n_batches))\n",
    "                    \n",
    "            train_loss = epoch_loss / n_batches\n",
    "            print_output = [epoch, train_loss]\n",
    "            if val_loader:\n",
    "                val_loss = self.validate(val_loader, loss, metrics)\n",
    "                for i in val_loss: print_output.append(i)\n",
    "\n",
    "            print(print_output)\n",
    "        if save:\n",
    "            state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': self.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}\n",
    "            self.save_checkpoint(state, filename=save_path)\n",
    "\n",
    "    def validate(self, val_loader, loss, metrics=None):\n",
    "        self.eval()\n",
    "        n_batches = int(val_loader.dataset.N / val_loader.batch_size)\n",
    "        total_loss = 0.\n",
    "        metric_scores = {}\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                metric_scores[str(metric)] = []\n",
    "                \n",
    "        for i, (batch_cat, batch_cont, batch_y) in enumerate(val_loader):\n",
    "            batch_cat, batch_cont, batch_y = Variable(batch_cat), Variable(batch_cont), Variable(batch_y)\n",
    "            y_hat = self.forward(batch_cat, batch_cont)\n",
    "            l = loss(y_hat, batch_y)\n",
    "            total_loss += l.data[0]\n",
    "            \n",
    "            if metrics:\n",
    "                for metric in metrics:\n",
    "                    metric_scores[str(metric)].append(metric(batch_y.data.numpy(), y_hat.data.numpy()))\n",
    "        if metrics:\n",
    "            final_metrics = []\n",
    "            for metric in metrics:\n",
    "                final_metrics.append(np.sum(metric_scores[str(metric)]) / n_batches)\n",
    "            return total_loss / n_batches, final_metrics\n",
    "        else:\n",
    "            return total_loss / n_batches\n",
    "        \n",
    "        \n",
    "    def save_checkpoint(self, state, filename='checkpoint.pth.tar'):\n",
    "        torch.save(state, filename)\n",
    "        \n",
    "    def predict(self, df, cat_flds, cont_flds):\n",
    "        self.eval()\n",
    "        cats = np.asarray(df[cat_flds], dtype=np.int64)\n",
    "        conts = np.asarray(df[cont_flds], dtype=np.float32)\n",
    "        x_cat = Variable(torch.from_numpy(cats))\n",
    "        x_cont = Variable(torch.from_numpy(conts))\n",
    "        pred = self.forward(x_cat, x_cont)\n",
    "        return pred.data.numpy().flatten()\n",
    "\n",
    "    \n",
    "# TODO\n",
    "# - learning rate restarts\n",
    "# change PATH for saved model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample, y_sample, nas, mapper = preprocessing(df_raw[:10000], 'SalePrice', do_scale=True)\n",
    "\n",
    "n_valid = 1000\n",
    "n_trn = len(df_sample)-n_valid\n",
    "val_idx = list(range(n_trn, len(df_sample)))\n",
    "\n",
    "train_loader, val_loader = StructuredData(df_sample, y_sample, cat_flds, cont_flds, batch_size=64, val_index=val_idx).get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_range=(0,np.max(y)*1.2)\n",
    "my_model = MultiInputNN(emb_szs, len(cont_flds), 0.05, 1, [500,250], [0.5,0.05], use_bn=True, y_range=y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1.7231188039694514, 0.4903570353984833, [0.7195496877034505, 0.07632531918010448]]\n",
      "[2, 0.3590959183871746, 0.27899246116479237, [0.5417129516601562, 0.5061897871685839]]\n"
     ]
    }
   ],
   "source": [
    "my_model.fit(train_loader, epochs=2, save=True, val_loader=val_loader, metrics=[rmse, r2_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...restoring model...\n",
      "[3, 0.2335229481969561]\n",
      "[4, 0.1690023023635149]\n"
     ]
    }
   ],
   "source": [
    "my_model.fit(train_loader, epochs=2, save=True, pre_saved=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...restoring model...\n",
      "[5, 0.12643333577683993, 0.13297265817721685, [0.3751620610555013, 0.8006963493330007]]\n",
      "[6, 0.1058651225907462, 0.12605141748984655, [0.36430749893188474, 0.8157269626822686]]\n",
      "[7, 0.09485879724047014, 0.0968967025478681, [0.3184300740559896, 0.876090542939196]]\n",
      "[8, 0.08323746100068093, 0.0922976424296697, [0.3111626942952474, 0.8833985765837554]]\n",
      "[9, 0.07922754921019078, 0.09525569528341293, [0.31697184244791665, 0.8774915622266801]]\n",
      "[10, 0.07112292892166547, 0.09411198894182841, [0.3144283930460612, 0.8804460592590257]]\n"
     ]
    }
   ],
   "source": [
    "my_model.fit(train_loader, epochs=6, save=True, pre_saved=True, val_loader=val_loader, metrics=[rmse, r2_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = my_model.predict(df_sample, cat_flds, cont_flds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24940509011324333"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(y_sample, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
